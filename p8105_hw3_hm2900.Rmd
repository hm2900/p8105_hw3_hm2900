---
title: "p8105_hw3_hm2900"
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(ggridges)
library(patchwork)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

### Load the data:

```{r}
library(p8105.datasets)
data("instacart")

instacart = 
  instacart %>% 
  as_tibble(instacart)
```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns, with each row resprenting a single product from an instacart order. Variables include identifiers for user, order, and product; the order in which each product was added to the cart. There are several order-level variables, describing the day and time of the order, and number of days since prior order. Then there are several item-specific variables, describing the product name (e.g. Yogurt, Avocado), department (e.g. dairy and eggs, produce), and aisle (e.g. yogurt, fresh fruits), and whether the item has been ordered by this user in the past. In total, there are `r instacart %>% select(product_id) %>% distinct %>% count` products found in `r instacart %>% select(user_id, order_id) %>% distinct %>% count` orders from `r instacart %>% select(user_id) %>% distinct %>% count` distinct users.


### How many aisles are there, and which aisles are the most items ordered from?

```{r}
instacart %>%
  summarize(
    n_aisle = n_distinct(aisle))

instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

There are 134 aisles. The most items are ordered from the aisle 'fresh vegetables' and 'fresh fruits'.  


### Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered.

```{r message = FALSE}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(
    title = "Number of items ordered in each aisle",
    x = "Aisle name",
    y = "Number of items",
    caption = "Data from p8105 package; The Instacart Online Grocery Shopping Dataset 2017."
  ) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```


### Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.

```{r message = FALSE}
instacart %>%
  group_by(aisle) %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  count(product_name) %>% 
  mutate(popular_rank = min_rank(desc(n))) %>%
  filter(popular_rank %in% c(1, 2, 3)) %>%
  arrange(desc(n)) %>%
  knitr::kable()
```


### Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r message = FALSE}
instacart %>%
  group_by(product_name, order_dow) %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  select(product_name, order_dow, mean_hour) %>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>%
  knitr::kable()
```

## problem 2

### Load the data:

```{r message = FALSE}
accel_data = 
  read_csv("./accel_data.csv") %>%
  janitor::clean_names() %>%
  mutate(weekend = ifelse(day %in% c("Saturday", "Sunday"), 1, 0)) %>%
  select(week, day_id, day, weekend, everything())

accel_data
```

There are `r ncol(accel_data)` variables and `r nrow(accel_data)` observations in this dataset. Totally there are 5 weeks and each week contains 7 different days with the activity counts for each minute of a 24-hour day starting at midnight. The variable `r colnames(accel_data)[3]` has the value of 0 if the day is a weekday and value of 1 if the day is a weekend. 


### Using your tidied dataset, aggregate across minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?

```{r message = FALSE}
accel_data %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity_minute",
    values_to = "activities"
  ) %>%
  group_by(week, day_id, day, weekend) %>%
  summarize(total_activitiy = sum(activities, na.rm = TRUE)) %>%
  knitr::kable()
```

According to the table, we can see that the total activity number is highest on Sunday in week 1 and on Saturday in week 2, while lowest on Mondays for both of two weeks, which means that for the first two week, there may be more activities on the weekends instead of weekdays. However, for week 3, 4 and 5, the total activity number is highest on Monday, Wednesday, and Friday. Especially for week 4 and 5, the total activity numbers is lowest on Saturday. This indicate that for the further week 3, 4, and 5, there may trend to be more activities on weekdays instead of weekends.


### Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week.

```{r message = FALSE}
accel_data %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity_minute",
    values_to = "activities"
  ) %>%
  separate(activity_minute, into = c("activity", "minute"), sep = 9) %>%
  mutate(minute = as.numeric(minute)) %>%
  ggplot(aes(x = minute, y = activities, color = day)) + 
  geom_line() + 
  labs(
    title = "24-Hour Activity Plot",
    x = "Hour",
    y = "Activities",
    caption = "Data from accel_data.csv.") +
  scale_x_continuous(
    breaks = seq(0, 1440, 60),
    labels = c("12am", "1am", "2am", "3am", "4am", "5am", "6am", "7am", "8am", "9am", "10am", "11am", "12pm", "1pm", "2pm", "3pm", "4pm", "5pm", "6pm", "7pm", "8pm", "9pm", "10pm", "11pm", "12am")) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

Based on the plot, we can see that the maximum of accelerometer data is between 6am to 11pm over the course of the day, while the minimum is between 12am to 5am. Among the maximum accelerometer data period, there are two peaks existing, one is between 11am to 12 pm on Sunday, and another is between 8pm-10pm on Friday. These may indicate that people prefer to use more accelerometer data before sleeping and when having free time at noon.

## Problem 3

### Load the data;

```{r}
library(p8105.datasets)
data("ny_noaa")

ny_noaa
```

This dataset contains `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns, representing station id, date, and weather records from over 100,000 stations. Some of the key variables such as `r colnames(ny_noaa)[3]`, `r colnames(ny_noaa)[4]`, `r colnames(ny_noaa)[5]` describe the weather data of Precipitation(tenths of mm), Snowfall(mm), and Snow depth(mm). The variable `r colnames(ny_noaa)[6]` indicates the recorded maximum temperature (tenths of degrees C), and the variable `r colnames(ny_noaa)[7]` indicates the recorded minimum temperature (tenths of degrees C). Some of the values are missing as NA. When we considering about the statistical analysis of one type of weather data (such as the mean, the distribution, etc.), the missing values would be an issue.

### For snowfall, what are the most commonly observed values? Why?

```{r message = FALSE}
ny_noaa =
  ny_noaa %>%
  as_tibble(ny_noaa) %>%
  janitor::clean_names() %>%
  separate(date, into = c("year", "month", "day")) %>%
  mutate(year = as.numeric(year),
         month = as.numeric(month),
         day = as.numeric(day))

ny_noaa %>%
  count(snow) %>% 
  arrange(desc(n))
```

The most commonly observed values are 0. This is sensible since most of the days do not have snow across years.

### Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

```{r message = FALSE}
ny_noaa %>%
  filter(month %in% c(1, 7)) %>%
  drop_na(tmax) %>%
  mutate(tmax = as.numeric(tmax)) %>%
  mutate(month = recode(month, `1` = "January", `7` = "July")) %>%
  group_by(id, year, month) %>%
  summarize(avg_tmax = mean(tmax, na.rm = TRUE)) %>%
  ggplot(aes(x = id, y = avg_tmax, color = year)) + 
  geom_point() +
  labs(
    title = "Average Max Temperature in January and in July in Each Station across Years",
    x = "Stations",
    y = "Average Max Temperature (C)",
    caption = "Data from p8105 package; NOAA accessed from the NOAA National Climatic Data Center 2017.") + 
  theme(axis.text.x=element_blank()) +
  facet_grid(. ~ month)
```

The average max temperature in January in each station across years is around 0C, with an approximate range between -100C to 100C, and it seems get higher as year increasing. The average max temperature in July is around 250C, with an approximate range between 200C to 320C. There are some outliers in January lower than -100C and higher than 100C before year 2000. There are extreme outliers in July which is lower than 150C before year 1990, while most of the outliers in July are higher than 320C after year 2000. 

### Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r message = FALSE}
tmax_tmin_plot = 
  ny_noaa %>%
  filter(!is.na(tmin),!is.na(tmax)) %>%
  mutate(tmax = as.numeric(tmax)) %>%
  mutate(tmin = as.numeric(tmin)) %>%
  ggplot(aes(x = tmin, y = tmax)) +
  geom_hex() +
  labs(
    title = "Max vs Min Temperature",
    x = "Min temperature (C)",
    y = "Max temperature (C)") +
  theme(legend.title = element_text(size = 9), 
        legend.text  = element_text(size = 5))

snowfall_plot = 
  ny_noaa %>%
  filter(!is.na(snow), snow > 0 & snow <100) %>%
  ggplot(aes(x = snow, y = as.factor(year), fill = year)) +
  geom_density_ridges(alpha = 0.5) +
  labs(
    title = "Distribution of Snowfall by Year",
    x = "Snowfall (mm)",
    y = "Year")

tmax_tmin_plot + snowfall_plot
```










